{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triathlon Data - Preparing Dashboard\n",
    "\n",
    "### context\n",
    "This datas are giving a context to the relevance of our porject.\n",
    "Triathlon is the combination of the three endurance sports: swim - bike - run and is one of the most growing discipline of the last 30 years.\n",
    "First mentioned in the 20th of 19th century in France as \"les trois sports\" it got forgotten for decades.\n",
    "Only in 1978 it was reinvented in Hawaii where the anual Ironman still can be named as the mother of this sport.\n",
    "\n",
    "### aim\n",
    "analogical to our dashboard for training athletes we want a interactive dashboard for the story of Triathlon from 1983 to 2022. \n",
    "\n",
    "### process\n",
    "for making it interactive via parameters in tableau, we build some new columns: \n",
    "- year\n",
    "- distance = full, half, olympic, sprint\n",
    "- special_category = Professional Athletes, Para Athletes, Higher Weight Athletes, Relay\n",
    "- new age classes = grouped 6 age ranges from below 19 to over 75\n",
    "\n",
    "Espacially the categorical and the new age class assessment was complex due to many different person_event_group entries \n",
    "\n",
    "### results\n",
    "\n",
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "import re\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. reading data from database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, types\n",
    "from sqlalchemy import text # to be able to pass string\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from dotenv import dotenv_values # to load the data from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data from .env file\n",
    "config = dotenv_values()\n",
    "\n",
    "# define variables for the login\n",
    "pg_user = config['AZURE_USER'] \n",
    "pg_host = config['AZURE_HOST']\n",
    "pg_port = config['AZURE_PORT']\n",
    "pg_db = config['AZURE_DB']\n",
    "pg_schema = config['AZURE_SCHEMA']\n",
    "pg_pass = config['AZURE_PASS']\n",
    "\n",
    "# build the URL\n",
    "url = f'postgresql://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}'\n",
    "\n",
    "# create the engine\n",
    "engine = create_engine(url, echo=False)\n",
    "engine.url\n",
    "\n",
    "# build the search path\n",
    "my_schema = pg_schema \n",
    "with engine.begin() as conn: \n",
    "    result = conn.execute(text(f'SET search_path TO {my_schema};'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_db_connection(engine):\n",
    "    try:\n",
    "        connection = engine.connect() # including 'connection' as variable to close the connection\n",
    "        print(\"Connection successful!\")\n",
    "        connection.close() # closing the connection\n",
    "        return True\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test the connection\n",
    "test_db_connection(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load DataFrame from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.read_sql(sql=text('SELECT * FROM public.df_tri_stats_clean;'), con=engine)\n",
    "\n",
    "df_stats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'year' as new column\n",
    "\n",
    "this will be the radial axis for the gender-plot in tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not all years are extractable by the last 4 - so check first if there is a '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year out of the event_link string\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_year(link):\n",
    "    if '-' in link[-4:]:\n",
    "        return link[-10:-6]  # if the date is in Format YYYY-MM-DD \n",
    "    match = re.search(r'/(\\d{4})$', link)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "df_stats['year'] = df_stats['event_link'].apply(extract_year)\n",
    "\n",
    "df_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for using in tableau we need it as a dateformat,so that we bring the 1st january as month and day\n",
    "\n",
    "df_stats['year'] = pd.to_datetime(df_stats['year'].astype(str) + '-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_link</th>\n",
       "      <th>gender</th>\n",
       "      <th>person_link</th>\n",
       "      <th>person_flag</th>\n",
       "      <th>person_event_group</th>\n",
       "      <th>person_event_swim_time</th>\n",
       "      <th>person_event_cycle_time</th>\n",
       "      <th>person_event_run_time</th>\n",
       "      <th>person_event_t1_time</th>\n",
       "      <th>person_event_t2_time</th>\n",
       "      <th>person_event_finish_time</th>\n",
       "      <th>person_country</th>\n",
       "      <th>year</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/rus/result/ironman/ireland-cork/full/2019</td>\n",
       "      <td>M</td>\n",
       "      <td>/irl/profile/halliwell-mark</td>\n",
       "      <td>IRL</td>\n",
       "      <td>M45-49</td>\n",
       "      <td>0</td>\n",
       "      <td>28325000000000</td>\n",
       "      <td>19594000000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1140000000000</td>\n",
       "      <td>49059000000000</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>2019-01-01 01:00:00-01:00</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/rus/result/ironman/ireland-cork/full/2019</td>\n",
       "      <td>F</td>\n",
       "      <td>/usa/profile/harris-polly</td>\n",
       "      <td>USA</td>\n",
       "      <td>F50-54</td>\n",
       "      <td>0</td>\n",
       "      <td>28733000000000</td>\n",
       "      <td>19329000000000</td>\n",
       "      <td>0</td>\n",
       "      <td>997000000000</td>\n",
       "      <td>49060000000000</td>\n",
       "      <td>United States</td>\n",
       "      <td>2019-01-01 01:00:00-01:00</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/rus/result/ironman/ireland-cork/full/2019</td>\n",
       "      <td>M</td>\n",
       "      <td>/fra/profile/peugeot-rodolphe</td>\n",
       "      <td>FRA</td>\n",
       "      <td>M25-29</td>\n",
       "      <td>0</td>\n",
       "      <td>28625000000000</td>\n",
       "      <td>19810000000000</td>\n",
       "      <td>0</td>\n",
       "      <td>640000000000</td>\n",
       "      <td>49075000000000</td>\n",
       "      <td>France</td>\n",
       "      <td>2019-01-01 01:00:00-01:00</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/rus/result/ironman/ireland-cork/full/2019</td>\n",
       "      <td>F</td>\n",
       "      <td>/gbr/profile/crawford-lisa</td>\n",
       "      <td>GBR</td>\n",
       "      <td>F50-54</td>\n",
       "      <td>0</td>\n",
       "      <td>27887000000000</td>\n",
       "      <td>20421000000000</td>\n",
       "      <td>0</td>\n",
       "      <td>771000000000</td>\n",
       "      <td>49079000000000</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2019-01-01 01:00:00-01:00</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/rus/result/ironman/ireland-cork/full/2019</td>\n",
       "      <td>M</td>\n",
       "      <td>/usa/profile/brumit-aaron</td>\n",
       "      <td>USA</td>\n",
       "      <td>M45-49</td>\n",
       "      <td>0</td>\n",
       "      <td>25958000000000</td>\n",
       "      <td>21110000000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2041000000000</td>\n",
       "      <td>49109000000000</td>\n",
       "      <td>United States</td>\n",
       "      <td>2019-01-01 01:00:00-01:00</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   event_link gender  \\\n",
       "0  /rus/result/ironman/ireland-cork/full/2019      M   \n",
       "1  /rus/result/ironman/ireland-cork/full/2019      F   \n",
       "2  /rus/result/ironman/ireland-cork/full/2019      M   \n",
       "3  /rus/result/ironman/ireland-cork/full/2019      F   \n",
       "4  /rus/result/ironman/ireland-cork/full/2019      M   \n",
       "\n",
       "                     person_link person_flag person_event_group  \\\n",
       "0    /irl/profile/halliwell-mark         IRL             M45-49   \n",
       "1      /usa/profile/harris-polly         USA             F50-54   \n",
       "2  /fra/profile/peugeot-rodolphe         FRA             M25-29   \n",
       "3     /gbr/profile/crawford-lisa         GBR             F50-54   \n",
       "4      /usa/profile/brumit-aaron         USA             M45-49   \n",
       "\n",
       "   person_event_swim_time  person_event_cycle_time  person_event_run_time  \\\n",
       "0                       0           28325000000000         19594000000000   \n",
       "1                       0           28733000000000         19329000000000   \n",
       "2                       0           28625000000000         19810000000000   \n",
       "3                       0           27887000000000         20421000000000   \n",
       "4                       0           25958000000000         21110000000000   \n",
       "\n",
       "   person_event_t1_time  person_event_t2_time  person_event_finish_time  \\\n",
       "0                     0         1140000000000            49059000000000   \n",
       "1                     0          997000000000            49060000000000   \n",
       "2                     0          640000000000            49075000000000   \n",
       "3                     0          771000000000            49079000000000   \n",
       "4                     0         2041000000000            49109000000000   \n",
       "\n",
       "   person_country                      year distance  \n",
       "0         Ireland 2019-01-01 01:00:00-01:00     full  \n",
       "1   United States 2019-01-01 01:00:00-01:00     full  \n",
       "2          France 2019-01-01 01:00:00-01:00     full  \n",
       "3  United Kingdom 2019-01-01 01:00:00-01:00     full  \n",
       "4   United States 2019-01-01 01:00:00-01:00     full  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'distance' as new column\n",
    "\n",
    "this column will be an interactive parameter in our tableau dashborad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats['event_link'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_stats['event_link'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_distance(link):\n",
    "    parts = link.split('/')\n",
    "    return parts[-2] if len(parts) > 1 else None\n",
    "\n",
    "df_stats['distance'] = df_stats['event_link'].apply(extract_distance)\n",
    "\n",
    "df_stats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The interactive paramaters will be: {df_stats[\"distance\"].unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep: event groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats['person_event_group'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_stats['person_event_group'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### problem: empty person_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for number of None in person_link\n",
    "\n",
    "df_stats['person_link'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are > 66,000 participants over the years without person_link.\n",
    "\n",
    "Before simply dropping them, I want to look for a pattern - maybe it's allways the same event over the years or it is just some nations. To then decide what to do with these data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe only with empty person_link\n",
    "\n",
    "df_tri_noPerson=df_stats[df_stats['person_link'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many years\n",
    "\n",
    "print(f\"There are {df_tri_noPerson['year'].nunique()} years with unclear athletes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means, it is a phenomenon over the total period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for the number of events (once per year could be a sign for always the same)\n",
    "\n",
    "print(f\"There are {df_tri_noPerson['event_link'].nunique()} events with unclear athletes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it's not only one event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tri_noPerson.head(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I'm heading for the age-group and maybe later for the person_country I check now how many are missing both of these information. In person_event_group 'M' and 'F' are not precise enough. Again, maybe that is due to a specific event (extreme-triathlon). I will check the coinstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first looking for country=None\n",
    "\n",
    "df_tri_noPerson['person_country'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So most of the doubtful rows don't give information for the country anyway. But let's check for the age-groups by the event_group. I assess M & F equaly to None, maybe there are more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the list of event_groups. \n",
    "\n",
    "list(df_tri_noPerson['person_event_group'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following seem suspecious too: 'MNKNOWN'\n",
    "\n",
    "So I would count 'None' + 'M' + 'F' + 'MNKNOWN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tri_drop = df_tri_noPerson[\n",
    "    df_tri_noPerson['person_event_group'].isna() | \n",
    "    df_tri_noPerson['person_event_group'].isin(['M', 'F', 'MNKNOWN'])\n",
    "]\n",
    "df_tri_drop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I could just drop these 9,368 rows.\n",
    "\n",
    "But I still would have 58.000 that I could not use for person_country. \n",
    "\n",
    "For deciding on that, I would like to calcuate the impact on the total data. If I drop them all I reduce the nunique() of person_event by just one. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of rows to drop would be {len(df_tri_noPerson)}, \"\n",
    "      f\"\\nthat is {(df_tri_noPerson['person_link'].nunique()) / (df_stats['person_link'].nunique()):.5%} of all athletes\"\n",
    "      f\"\\nbut {len(df_tri_noPerson) / len(df_stats):.2%} of all participations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure if I will keep them in the end. For now I'll only drop those with unclear event_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = df_stats.merge(df_tri_drop, how=\"left\", indicator=True)\n",
    "df_stats = df_stats[df_stats[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### problem: unclear groups\n",
    "\n",
    "There are 966 different groups - depending on event, the grouping varies. \n",
    "\n",
    "- Pro, Para, Relay and Clydesdale+Athena are not grouped by ages, but I will use them as a new column for special categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for all groups without clear numbers in it\n",
    "\n",
    "def filter_entries_without_numbers(df, column_name):\n",
    "    # filter for numbers ^over total string, \\D+ only numbers, $ no number in the end\n",
    "    return df[df[column_name].str.contains(r'^\\D+$', na=False)]\n",
    "\n",
    "# use function\n",
    "filtered_df = filter_entries_without_numbers(df_stats, 'person_event_group')\n",
    "\n",
    "# show result\n",
    "list(filtered_df['person_event_group'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categories  para / pro / A&C /relay\n",
    "\n",
    "By manualy sorting the groups, I will build a new column with categories para, pro, A&C and relay - this will be a parameter in tableau\n",
    "\n",
    "- para incl PTV (=blind), PTWC (): manualy sorted\n",
    "\n",
    "\n",
    "- overweight A&C: Athena = female above 165 pounds (74,5kg) + Clydesdale =  male above 220 pounds (99,7kg): manualy sorted\n",
    "\n",
    "\n",
    "- relay: \n",
    "\n",
    "\n",
    "- pro: search for pro in string\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for preparing the categorical column - get the total of pro\n",
    "\n",
    "df_stats[df_stats[\"person_event_group\"].str.contains(\"pro\", case=False, na=False)][\"person_event_group\"].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build new column with categories\n",
    "\n",
    "\n",
    "para_list = [\n",
    "    'MPARA', 'MPARATHLETE', 'FPARATHLETE', 'MПАРААТЛЕТ', 'FПАРААТЛЕТ', 'PARATHLETE', 'TPARA', \n",
    "    'MPARAATHLETE', 'MPARATRIATHLETE', 'MPARATRI', 'FPARA', 'MAWADUPPEREXTREMITYBELOWELBOW', \n",
    "    'MAWADBLIND', 'MAWADOTHER', 'MAWADHANDCYCLE', 'MAWADUPPEREXTREMITY+ELBOW', \n",
    "    'MAWADLOWEREXTREMITYBELOWKNEE', 'FAWADLOWEREXTREMITYBELOWKNEE', 'FAWADHANDCYCLE', \n",
    "    'MAWADLOWEREXTREMITY+KNEE', 'FAWADBLIND', 'FAWADWHEELCHAIR', 'FAWADOTHER', \n",
    "    'MAWADDOUBLEAMPUTEEBELOWKNEE', 'FAWADUPPEREXTREMITYBELOWELBOW', 'MHANDCYCLE', 'MPTVI', \n",
    "    'FPTVI', 'MPTWC', 'FPTWC', 'FPARATRIAHLON40-49', 'MPTS1', 'MPTS2', 'MPTS3', 'MPTS4', 'MPTS5', 'MPTS5OPEN',\n",
    "    'FPTS1', 'FPTS2', 'FPTS3', 'FPTS4', 'FPTS5', 'FPTS5OPEN',\n",
    "    'MPTVI', 'FPTVI','MPTWC', 'FPTWC', 'MPTWCOPEN', 'FPTWCOPEN', 'PTVI', 'MPYB', 'FPYB',  'MPTWCOPEN', 'MPTVIOPEN','FPTWCOPEN', 'MPTS5OPEN',\n",
    "    'MPTWCOPEN','MPTVIOPEN','MPTS4OPEN','MPTS2OPEN','MPTS3OPEN','FPTWCOPEN','MPT4OPEN','MPT5OPEN',\n",
    "    'MPT1OPEN','MPT2OPEN','MPT3OPEN','FPT5OPEN','FPT1OPEN','FPT3OPEN', 'MPTTRI-1','MPTTRI-6A',\n",
    "    'MPTTRI-2','FPTTRI-6B','FPTTRI-4','FPTTRI-1','FPTTRI-3','FPTTRI-5','FPTTRI-6A','FPTTRI-2','MPTTRI-6','FPTTRI-6', 'MPT1',\n",
    "    'MPT5','MPT4','MPT2','FPT5','FPT4','MPT3','FPT1','MPTTRI-5','ПАРААТЛЕТ',  'MAWADWHEELCHAIR1','MAWADWHEELCHAIR2', 'MPTTRI-4',\n",
    "    'MPTTRI-6B','MPTTRI-3', 'FAWADPC3','MAWADPC4','MAWADPC5','MAWADPC3','FAWADPC6','FAWADPC5','MAWADPC2','MAWADPC1',\n",
    "    'FAWADPC2','MAWADPC6','FAWADPC4', 'FPC', 'MPC', 'FAWADPC1',  'PT5','PT4','PT3',  'FPT2','FPT3'\n",
    "    ]\n",
    "\n",
    "ac_list = ['MCLYDESDALE', 'FCLYDESDALE', 'FATHENAS', 'MCLYDESDALES', 'MCLY', 'FCLY', 'MCLY-U', 'MCLY-O','FATH', 'FATH-U', 'FATH-O']\n",
    "\n",
    "relay_list = ['MRELAY', 'FRELAY', 'RELAY', 'MRELAYCOED', 'MIXED', 'M-RELAY', 'M-RLY', 'FX-RLY', 'F-RLY', 'MX-RLY', \n",
    "              'FRELAYCOED', 'MTEAM', 'FTEAM', 'FCOUPLES','MCOUPLES']\n",
    "\n",
    "pro_list = ['MPRO',\n",
    "    'FPRO','MPROEN','MFPROO','MSEMIFINAL3PRO','MSEMIFINAL2PRO','MSEMIFINAL1PRO','MPRODISTANCE','FPRODISTANCE','FSEMIFINAL3PRO',\n",
    "    'FSEMIFINAL1PRO','FSEMIFINAL2PRO','MAGMPRO','FAGFPRO','MSEMIFINAL4PRO','FSEMIFINAL4PRO','MPROSEMIFINAL3','MPROSEMIFINAL2',\n",
    "    'MPROSEMIFINAL1','FPROSEMIFINAL1','FPROSEMIFINAL2','MPRO30-34','MPRO40-44','MPRO25-29','MPRO35-39','FPRO30-34',\n",
    "    'FPRO35-39','FPROILIPINOPRO','MPROANDU23','FPROANDU23','MQUALIFIER2PRO','MQUALIFIER1PRO','MSEMIFINAL2MPROEN',\n",
    "    'MSEMIFINAL1MPROEN','MPRO1','MPROENSEMIFINAL2','MFPROOSEMIFINAL1','MQUALIFIER3PRO','MPROENSEMIFINAL3','MFPROOSEMIFINAL2',\n",
    "    'MQUALIFIER4MPROEN','MQUALIFIER6MPROEN','MSEMIFINAL3MPROEN','MSEMIFINAL1FPROO','FFINALBPRO','MFINALCPRO',\n",
    "    'MFINALBPRO','MFPROINALSTE1','MFPROINALSTE2','MFPROINALSTE3','FFPROINALSTE2','FFPROINALSTE1','FQUALIFIER1PRO',\n",
    "    'FQUALIFIER2PRO','FFPROINALSTE3','MREPECHAGE2MPROEN','MQUALIFIER2MPROEN','MQUALIFIER4FPROO','MQUALIFIER5MPROEN',\n",
    "    'MQUALIFIER1MPROEN','MREPECHAGE3MPROEN','MQUALIFIER2FPROO','MREPECHAGE1FPROO','MQUALIFIER3MPROEN','MQUALIFIER1FPROO',\n",
    "    'MREPECHAGE4MPROEN','MREPECHAGE2FPROO','MREPECHAGE1MPROEN','MQUALIFIER3FPROO','MPROENREPECHAGE1STAGE1',\n",
    "    'MPROENHEAT3STAGE1','MPROENREPECHAGE1STAGE2','MPROENHEAT3STAGE2','MPROENHEAT3','MFPROOREPECHAGE1STAGE1',\n",
    "    'MFPROOHEAT1STAGE1','MFPROOREPECHAGE1STAGE2','MFPROOHEAT1STAGE2','MFPROOHEAT1','MPROENHEAT1STAGE1',\n",
    "    'MPROENHEAT1STAGE2','MPROENHEAT1','MPROENREPECHAGE2STAGE1','MPROENREPECHAGE2STAGE2','MPROENHEAT2STAGE1',\n",
    "    'MPROENHEAT2STAGE2','MPROENHEAT2','MFPROOHEAT2STAGE1','MFPROOHEAT2STAGE2','MFPROOHEAT2','FREPECHE1PRO','MREPECHE2PRO',\n",
    "    'MREPECHE1PRO']\n",
    "\n",
    "\n",
    "# function for assigning categories\n",
    "def assign_category(person_event_group):\n",
    "    if person_event_group in para_list:\n",
    "        return 'para'\n",
    "    elif person_event_group in ac_list:\n",
    "        return 'a+c'\n",
    "    elif person_event_group in relay_list:\n",
    "        return 'relay'\n",
    "    elif person_event_group in pro_list:\n",
    "        return 'pro'\n",
    "    else:\n",
    "        return 'None'  # For the rows without\n",
    "\n",
    "# generate new column\n",
    "df_stats['special_category'] = df_stats['person_event_group'].apply(assign_category)\n",
    "\n",
    "df_stats.head(25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translate age groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for specific groups to see potential translation / assessment\n",
    "\n",
    "df_search_groups = df_stats[df_stats['person_event_group'].isin(['MYOUTH',\n",
    "'FYOUTH'])]\n",
    "\n",
    "df_search_groups['event_link'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### junior/youth as  <19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I group all Junior and Youth categories to one group '<19'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_young = df_stats[df_stats[\"person_event_group\"].str.contains(\"ju\", case=False, na=False)]\n",
    "list(df_young['person_event_group'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_young = df_stats[df_stats[\"person_event_group\"].str.contains(\"y\", case=False, na=False)]\n",
    "list(df_young['person_event_group'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate category <19\n",
    "\n",
    "df_stats['person_event_group'] = df_stats['person_event_group'].replace([\n",
    "    'MJUNIOR', 'FJUNIOR', 'MSEMIFINAL1JUNIOR', 'MSEMIFINAL3JUNIOR', 'MSEMIFINAL2JUNIOR', \n",
    "    'FSEMIFINAL2JUNIOR', 'FSEMIFINAL1JUNIOR', 'MJUNIOREN18+19', 'MTIMETRIALQUALIFIERJUNIOR', \n",
    "    'FTIMETRIALQUALIFIERJUNIOR', 'MSEMIFINAL4JUNIOR', 'MFINALJUNIOR', 'FFINALJUNIOR', \n",
    "    'MJUNIORB', 'MJUNIORA', 'FJUNIORB', 'FJUNIORA', 'FSEMIFINAL3JUNIOR', 'MFINALBJUNIOR', \n",
    "    'FFINALBJUNIOR', 'MFINALYOUTH', 'MFINALBYOUTH', 'FFINALYOUTH', 'FFINALBYOUTH', \n",
    "    'MSEMIFINAL1YOUTH', 'MSEMIFINAL3YOUTH', 'MSEMIFINAL2YOUTH', 'FSEMIFINAL3YOUTH', \n",
    "    'FSEMIFINAL1YOUTH', 'FSEMIFINAL2YOUTH', 'MYOUTH', 'FYOUTH', 'MTIMETRIALQUALIFIERYOUTH', \n",
    "    'FTIMETRIALQUALIFIERYOUTH','MJUN19-23', 'MJUN16-19', 'FJUN16-19', 'FJUN19-23', 'MJUN', 'FJUN', 'MJU', 'MJUN', 'YM18','YW18',\n",
    "    'YW16','YM16', 'M19UND','F19UND'\n",
    "], '<19')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### veterans of Deltebre\n",
    "\n",
    "Now we have to find translations for the not intuitive ones:\n",
    "\n",
    "- MVETERANS1, MVETERANS2, MVETERANS3, FVETERANS1, FVETERANS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for the MVETERANS\n",
    "# which event uses these categories to then research for the meaning\n",
    "\n",
    "df_veterans = df_stats[df_stats[\"person_event_group\"].str.contains(\"veterans\", case=False, na=False)]\n",
    "list(df_veterans['event_link'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So they MVETERANS come from the TriTour Event in Deltebre in Catalonia.\n",
    "A websearch could calrify :\n",
    "- Youth = 16-17 years\n",
    "- Junior = 18-19 years\n",
    "- U23 = 20-23 years\n",
    "- Senior = 24-39 years\n",
    "- Veterans 1 = 40-49 years\n",
    "- Veterans 2 = 50-59 years\n",
    "- Veterans 3 = above 60 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the entrys\n",
    "\n",
    "df_stats['person_event_group'] = df_stats['person_event_group'].replace({\n",
    "    'MVETERANS1': '40-49',\n",
    "    'FVETERANS1': '40-49',\n",
    "    'MVETERANS2': '50-59',\n",
    "    'FVETERANS2': '50-59',\n",
    "    'MVETERANS3': '>60'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### senior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senior = df_stats[df_stats[\"person_event_group\"].str.contains(\"senior\", case=False, na=False)]\n",
    "list(df_senior['person_event_group'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are some categories for seniors - I'll try to get information on of the numbered may belong to one particular event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_groups = [\n",
    "    'MSENIOR4', 'MSENIOR2', 'MSENIOR3', 'MSENIOR1',\n",
    "    'FSENIOR4', 'FSENIOR3', 'FSENIOR2', 'FSENIOR1'\n",
    "]\n",
    "\n",
    "df_seniors = df_stats[df_stats['person_event_group'].isin(senior_groups)]\n",
    "df_seniors['event_link'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web research for Rimini Triathlon:\n",
    "\n",
    "- Under23\t20 - 23\n",
    "- Senior 1\t20 - 24\n",
    "- Senior 2\t25 - 29\n",
    "- Senior 3\t30 - 34\n",
    "- Senior 4\t35 - 39\n",
    "- Master 1\t40 - 44\n",
    "- Master 2\t45 - 49\n",
    "- Master 3\t50 - 54\n",
    "- Master 4\t55 - 59\n",
    "- Master 5\t60 - 64\n",
    "- Master 6\t65 - 69\n",
    "- Master 7\t70 - 74\n",
    "- Master 8\t75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the entrys\n",
    "\n",
    "df_stats['person_event_group'] = df_stats['person_event_group'].replace({\n",
    "    'MSENIOR1': '20-24', \n",
    "    'FSENIOR1': '20-24',\n",
    "    'MSENIOR2': '25-29', \n",
    "    'FSENIOR2': '25-29',\n",
    "    'MSENIOR3': '30-34', \n",
    "    'FSENIOR3': '30-34',\n",
    "    'MSENIOR4': '35-39', \n",
    "    'FSENIOR4': '35-39'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but still there are 'senior'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senior = df_stats[df_stats[\"person_event_group\"].str.contains(\"senior\", case=False, na=False)]\n",
    "df_senior['event_link'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we know Delterbe already and I did a research for the politie triathlon in the Netherlands. This event gives some problems anyway, let's have a closer look on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politie = df_stats[df_stats[\"event_link\"].str.contains(\"politie\", case=False, na=False)]\n",
    "df_senior['person_link'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I will use the translation of Delterbe and drop the politie event with 112 athletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop politie\n",
    "\n",
    "df_stats = df_stats[~df_stats[\"event_link\"].str.contains(\"politie\", case=False, na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate senior according to Delterbe\n",
    "\n",
    "df_stats['person_event_group'] = df_stats['person_event_group'].replace({\n",
    "    'MSENIOR': '24-39', \n",
    "    'FSENIOR': '24-39'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### masters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for the MASTERS\n",
    "# which event uses these categories to then research for the meaning\n",
    "\n",
    "df_master = df_stats[df_stats[\"person_event_group\"].str.contains(\"master\", case=False, na=False)]\n",
    "\n",
    "print(\"The events are:\")\n",
    "list(df_master['event_link'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by event_link than aggregate the list of category names and another list with one entry the number of individual athletes\n",
    "grouped = df_master.groupby('event_link').agg(\n",
    "    unique_groups=('person_event_group', 'unique'),\n",
    "    unique_person_links=('person_link', 'nunique')\n",
    ")\n",
    "\n",
    "# iterate through the rows \n",
    "print(\"The events with their corresponding group names and unique person counts are: \\n\")\n",
    "for event, row in grouped.iterrows():\n",
    "    print(f\"Event: {event}\\nGroups: {', '.join(row['unique_groups'])}\\nUnique person links: {row['unique_person_links']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know for Rimini:\n",
    "\n",
    "- Master 1\t40 - 44\n",
    "- Master 2\t45 - 49\n",
    "- Master 3\t50 - 54\n",
    "- Master 4\t55 - 59\n",
    "- Master 5\t60 - 64\n",
    "- Master 6\t65 - 69\n",
    "- Master 7\t70 - 74\n",
    "- Master 8\t75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the entrys\n",
    "\n",
    "df_stats['person_event_group'] = df_stats['person_event_group'].replace({\n",
    "    'FMASTERS1': '40-44',\n",
    "    'MMASTERS1': '40-44', \n",
    "    'FMASTERS2': '45-49',\n",
    "    'MMASTERS2': '45-49',  \n",
    "    'FMASTERS3': '50-54',\n",
    "    'MMASTERS3': '50-54',\n",
    "    'MMASTERS4': '55-59', \n",
    "    'MMASTERS5': '60-64',\n",
    "    'MMASTERS6': '65-69'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as I don't get information for the older two events but the total is only 120 athletes, I will drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of entries to drop\n",
    "exclude_groups = ['FMASTERSV1', 'MASTERSV1', \n",
    "                  'FMASTERSV2', 'MASTERSV2', \n",
    "                  'FMASTERSV3', 'MASTERSV3', \n",
    "                  'FMASTERSV4', 'MASTERSV4', \n",
    "                  'FMASTERSV5', 'MASTERSV5']\n",
    "\n",
    "# use list to drop\n",
    "df_stats = df_stats[~df_stats['person_event_group'].isin(exclude_groups)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for the groups - f.e. everything regarding 'open' (for para I had to check manualy)\n",
    "\n",
    "df_open = df_stats[df_stats[\"person_event_group\"].str.contains(\"open\", case=False, na=False)]\n",
    "list(df_open['person_event_group'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the open categories can either be traducted to an age or are already in the para-group. Only the MOPEN and FOPEN don't give any information at the moment. So let's look for then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_open = df_stats[df_stats[\"person_event_group\"].str.contains(\"MOPEN|FOPEN\", case=False, na=False)]\n",
    "\n",
    "print(df_open['event_link'].unique())\n",
    "\n",
    "print(f\"\\nThe number of athletes all over these events is {df_open['person_link'].nunique()}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will drop these athletes due to being 0.0001% of the total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop MOPEN and FOPEN\n",
    "\n",
    "df_stats = df_stats[~df_stats[\"person_event_group\"].str.contains(\"MOPEN|FOPEN\", case=False, na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what's left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat looking for unclear groups - only include the rows where 'special_category' is None\n",
    "\n",
    "\n",
    "def filter_entries_without_numbers(df, column_name):\n",
    "    # build filter where special_category is None\n",
    "    df = df[df['special_category']== 'None']\n",
    "    return df[df[column_name].str.contains(r'^\\D+$', na=False)]\n",
    "\n",
    "# use function\n",
    "filtered_df = filter_entries_without_numbers(df_stats, 'person_event_group')\n",
    "\n",
    "# show result\n",
    "print(f\"There are {filtered_df['person_event_group'].nunique()} unclear categories for \"\n",
    "      f\"{filtered_df['person_link'].nunique()} individual athletes\")\n",
    "list(filtered_df['person_event_group'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplary I check these entries\n",
    "\n",
    "- MWHITE, MBLUE, MBLACK, MYELLOW\n",
    "- FXTRI, MXTRI\n",
    "- FTBC, MTBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for the  MWHITE, MBLUE, MBLACK, MYELLOW\n",
    "# which event uses these categories to then research for the meaning\n",
    "\n",
    "df_colour= df_stats[df_stats[\"person_event_group\"].str.contains(\"MWHITE|MBLUE|MBLACK|MYELLOW\", case=False, na=False)]\n",
    "\n",
    "print(f\"There are {df_colour['person_link'].count()} participant in {df_colour['event_link'].nunique()} events.\\nThe list of events is \\n{df_colour['event_link'].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for FXTRI|MXTRI|FTBC|MTBC\n",
    "\n",
    "df_colour= df_stats[df_stats[\"person_event_group\"].str.contains(\"FXTRI|MXTRI|FTBC|MTBC\", case=False, na=False)]\n",
    "\n",
    "print(f\"There are {df_colour['person_link'].count()} participant in {df_colour['event_link'].nunique()} events.\\nThe list of events is \\n{df_colour['event_link'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these groups refer to different courses and don't give any hint on agegroup. As it is about 824 participants, I drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping FXTRI|MXTRI|FTBC|MTBC & MWHITE, MBLUE, MBLACK, MYELLOW\n",
    "\n",
    "# call the entries to be droped\n",
    "exclude_values = ['FXTRI', 'MXTRI', 'FTBC', 'MTBC', 'MWHITE', 'MBLUE', 'MBLACK', 'MYELLOW']\n",
    "\n",
    "# keep those that are not (~) including the excluded values\n",
    "df_stats = df_stats[~df_stats['person_event_group'].isin(exclude_values)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll check the amount of all the unclear groups to decide if to drop them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of athletes to drop would be {filtered_df['person_link'].nunique()}, \"\n",
    "      f\"\\nwhich is {filtered_df['person_link'].nunique() / df_stats['person_link'].nunique():.2%} of all athletes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with the 2.25% of rows without person_link, I had to drop about 5% of the data.\n",
    "But as I need the person_link for comparing if an athlete apears twice or more on an event, I obligatorily need the information out of this column.\n",
    "I will drop the empty person_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows without person_link\n",
    "\n",
    "df_stats = df_stats.dropna(subset=['person_link'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## athlets per event\n",
    "\n",
    "Some of these groups suggest, that an athlete may has changed his group from one to another during an event, f.e. from qualifier to finalist.\n",
    "\n",
    "To make sure that each athlete isn't counted more then once per event, I will check for duplicates depending on event_link and person_link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates again - maybe there was something wrong\n",
    "\n",
    "duplicates_s = df_stats[df_stats.duplicated(keep=False)]\n",
    "\n",
    "print('Number of duplicats: ', len(duplicates_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates again\n",
    "\n",
    "df_stats = df_stats.drop_duplicates(keep = 'first', ignore_index = True, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for double athletes on one event, due to changing the group\n",
    "\n",
    "df_double_event = df_stats[df_stats.duplicated(subset=['person_link', 'event_link'], keep=False)]\n",
    "\n",
    "# drop doubled combinations\n",
    "df_double_event = df_double_event.drop_duplicates(subset=['person_link', 'event_link', 'person_event_group'])\n",
    "\n",
    "# group to see wich person_event_link per person_link\n",
    "duplicates_grouped = df_double_event.groupby(['event_link', 'person_link'])['person_event_group'].apply(lambda x: list(set(x))).reset_index()\n",
    "\n",
    "\n",
    "print(f\"There are {len(duplicates_grouped)} entries on the list\"\n",
    "      f\"\\nand {duplicates_grouped['person_link'].nunique()} individual athletes\")\n",
    "duplicates_grouped.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers should be the same - as I group by the combination of one event and one athlete the total length and the number of indovidual athletes should be the same.\n",
    "\n",
    "I'll try to find the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wich person_link come up more then once\n",
    "\n",
    "duplicates_grouped['person_link'].value_counts().head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for the example of \"/aus/profile/jeffcoat-emma\" to get more insights\n",
    "\n",
    "df_double_event[df_double_event['person_link'] == \"/aus/profile/jeffcoat-emma\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how often on wich event\n",
    "\n",
    "df_double_event[df_double_event['person_link'] == \"/aus/profile/jeffcoat-emma\"]['event_link'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the reason for the difference between length and nunique() ist due to one athlete being doublede on several events. \n",
    "\n",
    "But is it sure that each athlete is realy at least twice on an event? I prefer to check that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count athletes person_link per event_link\n",
    "counts = df_double_event.groupby(['event_link', 'person_link']).size()\n",
    "\n",
    "# all group combis (event-person) that are at least twice\n",
    "valid_persons = counts[counts >= 2].reset_index()[['event_link', 'person_link']]\n",
    "\n",
    "# filter only the valid person\n",
    "df_double_event = df_double_event.merge(valid_persons, on=['event_link', 'person_link'], how='inner')\n",
    "\n",
    "duplicates_grouped = df_double_event.groupby(['event_link', 'person_link'])['person_event_group'].apply(list).reset_index()\n",
    "\n",
    "print(f\"The total number of rows is {len(df_double_event)} and can be grouped to a \\n\"\n",
    "      f\"list with {len(duplicates_grouped)} entries of {duplicates_grouped['event_link'].nunique()} events\"\n",
    "      f\"\\nfrom {duplicates_grouped['person_link'].nunique()} individual athletes\"\n",
    "      # check if each athlete is there at least two times\n",
    "      f\"\\neach athlete: {df_double_event.groupby(['event_link', 'person_link']).size().min()} times per event\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reasons for duplications\n",
    "\n",
    "now we can bring together the rows of one athlete per event, depending on the reason for duplicates:\n",
    "\n",
    "1. changing during the race (from qualifier to finalist / for youth & junior already solved, so that will happen during assessment to new overall age groups)\n",
    "\n",
    "2. including the special_category information to the age_group information (pro)\n",
    "\n",
    "3. two age-groups due to mistake (F & M) or due to in between two agegroups : search for the same athlete in other events, compare with year what age-group would be consequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. drop finalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During a race the athlete may have become a finalist. So all groups including *finalist get dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows that include the word *final (semifinalist, finalist..) - keep age-related group or the first final row\n",
    "\n",
    "# function that checks if there would remain a row without 'final' \n",
    "def resolve_final_entries(group):\n",
    "    # is there 'final' in the group \n",
    "    final_entries = group['person_event_group'].str.contains(\"final\", case=False, na=False)\n",
    "    # is there another entry without final\n",
    "    if final_entries.any() and (not final_entries.all()):\n",
    "        # Keep only rows that do not contain 'final'\n",
    "        return group[~final_entries]\n",
    "    # If all entries are 'final', keep only the first one\n",
    "    if final_entries.all():\n",
    "        return group.head(1)\n",
    "    # if no final, keep the group untouched\n",
    "    return group\n",
    "\n",
    "# use the filter function (apply as we want to drop rows)\n",
    "df_double_event = df_double_event.groupby(['event_link', 'person_link']).apply(resolve_final_entries).reset_index(drop=True)\n",
    "\n",
    "# compare to before\n",
    "duplicates_grouped = df_double_event.groupby(['event_link', 'person_link'])['person_event_group'].apply(list).reset_index()\n",
    "\n",
    "print(f\"The total number of rows is {len(df_double_event)} and can be grouped to a \\n\"\n",
    "      f\"list with {len(duplicates_grouped)} entries of {duplicates_grouped['event_link'].nunique()} events\"\n",
    "      f\"\\nfrom {duplicates_grouped['person_link'].nunique()} individual athletes\"\n",
    "      # check if each athlete is there at least two times\n",
    "      f\"\\neach athlete: {df_double_event.groupby(['event_link', 'person_link']).size().min()} times per event\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. special category\n",
    "\n",
    "if the athlete defined by 'person_link' is registered twice on an event, compare the 'special_category' and integrate\n",
    "this information to all the rows of this athlete in this event.\n",
    "After that dropping the rows of this athlete in this event where person_event_group is out of the list of special categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring the categorical information out of 'special_category' to all row per group (event+person)\n",
    "\n",
    "\n",
    "def resolve_special_category(group):\n",
    "    # build a list of all values that aren't 'None'\n",
    "    unique_values = [val for val in group.unique() if not pd.isna(val) and val != \"None\"] \n",
    "    \n",
    "    # in case of more then one special_category, mark it seperatly\n",
    "    if len(unique_values) > 1:\n",
    "        group[\"is_multiple_categories\"] = True  \n",
    "        return group  # skip replacement\n",
    "    \n",
    "    # in case of 'None' + one unique other value: replace\n",
    "    if \"None\" in group.values and unique_values:\n",
    "        return group.replace(\"None\", unique_values[0])  # unique_value instead of None\n",
    "    \n",
    "    return group  # if only None, nothing happens\n",
    "\n",
    "# by using .transform() it's used in each group (event_link and person_link) on each row\n",
    "df_double_event[\"special_category\"] = df_double_event.groupby([\"event_link\", \"person_link\"])[\"special_category\"].transform(resolve_special_category)\n",
    "\n",
    "# check if there are any cases of more then one special_category\n",
    "df_double_event[\"is_multiple_categories\"] = df_double_event.groupby([\"event_link\", \"person_link\"])[\"special_category\"].transform(\n",
    "    lambda x: len(x.unique()) > 1\n",
    ")\n",
    "\n",
    "df_multiple_categories = df_double_event[df_double_event[\"is_multiple_categories\"]]\n",
    "\n",
    "df_multiple_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to before\n",
    "duplicates_grouped = df_double_event.groupby(['event_link', 'person_link'])['person_event_group'].apply(list).reset_index()\n",
    "\n",
    "print(f\"The total number of rows is {len(df_double_event)} and can be grouped to a \\n\"\n",
    "      f\"list with {len(duplicates_grouped)} entries of {duplicates_grouped['event_link'].nunique()} events\"\n",
    "      f\"\\nfrom {duplicates_grouped['person_link'].nunique()} individual athletes\"\n",
    "      # check if each athlete is there at least two times\n",
    "      f\"\\neach athlete: {df_double_event.groupby(['event_link', 'person_link']).size().min()} times per event\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop the person_event_group that gave the additional information for the special_category. \n",
    "For curiosity I first have a look, what categorical information without age is there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person_event_groups in here without numbers\n",
    "\n",
    "def filter_entries_without_numbers(df, column_name):\n",
    "    # filter for numbers ^over total string, \\D+ only numbers, $ no number in the end\n",
    "    return df[df[column_name].str.contains(r'^\\D+$', na=False)]\n",
    "\n",
    "# use function\n",
    "filtered_df = filter_entries_without_numbers(df_double_event, 'person_event_group')\n",
    "\n",
    "# show result\n",
    "list(filtered_df['person_event_group'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I can't be sure that the special categories are doubled with another age-group category I will make sure not to loose athletes by dropping the categorical rows.\n",
    "\n",
    "(I will also have to think of that when building our new age-groups, that there will be a big group without information about age.)\n",
    "\n",
    "Nevertheless, I use the total special_list to drop the rows that we used for the information that are now transferred to the age-group relative row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of special categories\n",
    "\n",
    "special_list = [\n",
    "    # para\n",
    "    'MPARA', 'MPARATHLETE', 'FPARATHLETE', 'MПАРААТЛЕТ', 'FПАРААТЛЕТ', 'PARATHLETE', 'TPARA', \n",
    "    'MPARAATHLETE', 'MPARATRIATHLETE', 'MPARATRI', 'FPARA', 'MAWADUPPEREXTREMITYBELOWELBOW', \n",
    "    'MAWADBLIND', 'MAWADOTHER', 'MAWADHANDCYCLE', 'MAWADUPPEREXTREMITY+ELBOW', \n",
    "    'MAWADLOWEREXTREMITYBELOWKNEE', 'FAWADLOWEREXTREMITYBELOWKNEE', 'FAWADHANDCYCLE', \n",
    "    'MAWADLOWEREXTREMITY+KNEE', 'FAWADBLIND', 'FAWADWHEELCHAIR', 'FAWADOTHER', \n",
    "    'MAWADDOUBLEAMPUTEEBELOWKNEE', 'FAWADUPPEREXTREMITYBELOWELBOW', 'MHANDCYCLE', 'MPTVI', \n",
    "    'FPTVI', 'MPTWC', 'FPTWC', 'FPARATRIAHLON40-49', 'MPTS1', 'MPTS2', 'MPTS3', 'MPTS4', 'MPTS5', 'MPTS5OPEN',\n",
    "    'FPTS1', 'FPTS2', 'FPTS3', 'FPTS4', 'FPTS5', 'FPTS5OPEN',\n",
    "    'MPTVI', 'FPTVI','MPTWC', 'FPTWC', 'MPTWCOPEN', 'FPTWCOPEN', 'PTVI', 'MPYB', 'FPYB',  'MPTWCOPEN', 'MPTVIOPEN','FPTWCOPEN', 'MPTS5OPEN',\n",
    "    'MPTWCOPEN','MPTVIOPEN','MPTS4OPEN','MPTS2OPEN','MPTS3OPEN','FPTWCOPEN','MPT4OPEN','MPT5OPEN',\n",
    "    'MPT1OPEN','MPT2OPEN','MPT3OPEN','FPT5OPEN','FPT1OPEN','FPT3OPEN', 'MPTTRI-1','MPTTRI-6A',\n",
    "    'MPTTRI-2','FPTTRI-6B','FPTTRI-4','FPTTRI-1','FPTTRI-3','FPTTRI-5','FPTTRI-6A','FPTTRI-2','MPTTRI-6','FPTTRI-6', 'MPT1',\n",
    "    'MPT5','MPT4','MPT2','FPT5','FPT4','MPT3','FPT1','MPTTRI-5','ПАРААТЛЕТ',  'MAWADWHEELCHAIR1','MAWADWHEELCHAIR2', 'MPTTRI-4',\n",
    "    'MPTTRI-6B','MPTTRI-3', 'FAWADPC3','MAWADPC4','MAWADPC5','MAWADPC3','FAWADPC6','FAWADPC5','MAWADPC2','MAWADPC1',\n",
    "    'FAWADPC2','MAWADPC6','FAWADPC4', 'FPC', 'MPC', 'FAWADPC1',  'PT5','PT4','PT3',  'FPT2','FPT3',\n",
    "    # a&c\n",
    "    'MCLYDESDALE', 'FCLYDESDALE', 'FATHENAS', 'MCLYDESDALES', 'MCLY', 'FCLY', 'MCLY-U', 'MCLY-O','FATH', 'FATH-U', 'FATH-O',\n",
    "    # relay\n",
    "    'MRELAY', 'FRELAY', 'RELAY', 'MRELAYCOED', 'MIXED', 'M-RELAY', 'M-RLY', 'FX-RLY', 'F-RLY', 'MX-RLY', \n",
    "    'FRELAYCOED', 'MTEAM', 'FTEAM', 'FCOUPLES','MCOUPLES',\n",
    "    # pro\n",
    "    'MPRO',\n",
    "    'FPRO','MPROEN','MFPROO','MSEMIFINAL3PRO','MSEMIFINAL2PRO','MSEMIFINAL1PRO','MPRODISTANCE','FPRODISTANCE','FSEMIFINAL3PRO',\n",
    "    'FSEMIFINAL1PRO','FSEMIFINAL2PRO','MAGMPRO','FAGFPRO','MSEMIFINAL4PRO','FSEMIFINAL4PRO','MPROSEMIFINAL3','MPROSEMIFINAL2',\n",
    "    'MPROSEMIFINAL1','FPROSEMIFINAL1','FPROSEMIFINAL2','MPRO30-34','MPRO40-44','MPRO25-29','MPRO35-39','FPRO30-34',\n",
    "    'FPRO35-39','FPROILIPINOPRO','MPROANDU23','FPROANDU23','MQUALIFIER2PRO','MQUALIFIER1PRO','MSEMIFINAL2MPROEN',\n",
    "    'MSEMIFINAL1MPROEN','MPRO1','MPROENSEMIFINAL2','MFPROOSEMIFINAL1','MQUALIFIER3PRO','MPROENSEMIFINAL3','MFPROOSEMIFINAL2',\n",
    "    'MQUALIFIER4MPROEN','MQUALIFIER6MPROEN','MSEMIFINAL3MPROEN','MSEMIFINAL1FPROO','FFINALBPRO','MFINALCPRO',\n",
    "    'MFINALBPRO','MFPROINALSTE1','MFPROINALSTE2','MFPROINALSTE3','FFPROINALSTE2','FFPROINALSTE1','FQUALIFIER1PRO',\n",
    "    'FQUALIFIER2PRO','FFPROINALSTE3','MREPECHAGE2MPROEN','MQUALIFIER2MPROEN','MQUALIFIER4FPROO','MQUALIFIER5MPROEN',\n",
    "    'MQUALIFIER1MPROEN','MREPECHAGE3MPROEN','MQUALIFIER2FPROO','MREPECHAGE1FPROO','MQUALIFIER3MPROEN','MQUALIFIER1FPROO',\n",
    "    'MREPECHAGE4MPROEN','MREPECHAGE2FPROO','MREPECHAGE1MPROEN','MQUALIFIER3FPROO','MPROENREPECHAGE1STAGE1',\n",
    "    'MPROENHEAT3STAGE1','MPROENREPECHAGE1STAGE2','MPROENHEAT3STAGE2','MPROENHEAT3','MFPROOREPECHAGE1STAGE1',\n",
    "    'MFPROOHEAT1STAGE1','MFPROOREPECHAGE1STAGE2','MFPROOHEAT1STAGE2','MFPROOHEAT1','MPROENHEAT1STAGE1',\n",
    "    'MPROENHEAT1STAGE2','MPROENHEAT1','MPROENREPECHAGE2STAGE1','MPROENREPECHAGE2STAGE2','MPROENHEAT2STAGE1',\n",
    "    'MPROENHEAT2STAGE2','MPROENHEAT2','MFPROOHEAT2STAGE1','MFPROOHEAT2STAGE2','MFPROOHEAT2','FREPECHE1PRO','MREPECHE2PRO',\n",
    "    'MREPECHE1PRO'\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to make sure not to drop an athlete totaly if he/she has no other 'person_event_group' we have to include that to our code and validate for dropping first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop the rows if information could be transferred\n",
    "\n",
    "df_double_special = df_double_event\n",
    "\n",
    "# mark the rows tht include entries from special_list\n",
    "df_double_special['is_special'] = df_double_special['person_event_group'].isin(special_list)\n",
    "\n",
    "# make a subset with those groups that have more then one row\n",
    "multiple_rows_groups = df_double_special.groupby(['event_link', 'person_link']).filter(lambda x: len(x) > 1)\n",
    "\n",
    "# if there is more then one, prefer to keep the one without special_entry but keep at least one \n",
    "def resolve_special_cateory(group):\n",
    "    if group['is_special'].any(): # check for any entrys of special_list\n",
    "        group_non_special = group[~group['is_special']]  # take off rows with special_list entry\n",
    "        if len(group_non_special) > 0:  # if there is one without, keep that\n",
    "            group = group_non_special\n",
    "        else:  # if all rows are in special_list, keep the first\n",
    "            group = group.head(1)\n",
    "    return group\n",
    "\n",
    "# use filter on each group (apply as we want to change the structure)\n",
    "df_filtered = multiple_rows_groups.groupby(['event_link', 'person_link']).apply(resolve_special_cateory)\n",
    "\n",
    "# prepare the single rows that were excluded in multiple_rows_group\n",
    "df_single_row_groups = df_double_special[~df_double_special['person_link'].isin(multiple_rows_groups['person_link'])]\n",
    "\n",
    "# bring both groups together again\n",
    "df_double_event = pd.concat([df_filtered, df_single_row_groups])\n",
    "\n",
    "# due to concat reset index\n",
    "df_double_event = df_double_event.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to before\n",
    "duplicates_grouped = df_double_event.groupby(['event_link', 'person_link'])['person_event_group'].apply(list).reset_index()\n",
    "\n",
    "print(f\"The total number of rows is {len(df_double_event)} and can be grouped to a \\n\"\n",
    "      f\"list with {len(duplicates_grouped)} entries of {duplicates_grouped['event_link'].nunique()} events\"\n",
    "      f\"\\nfrom {duplicates_grouped['person_link'].nunique()} individual athletes\"\n",
    "      # check if each athlete is there at least two times\n",
    "      f\"\\neach athlete: {df_double_event.groupby(['event_link', 'person_link']).size().min()} times per event\")\n",
    "\n",
    "duplicates_grouped.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a wrong gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if gender and person_event_group are alliigned\n",
    "\n",
    "def resolve_gender_mismatch(group):\n",
    "    # extract the letters M or F from person_event_group\n",
    "    group['gender_letter'] = group['person_event_group'].str.extract(r'([MF])', expand=False)\n",
    "    \n",
    "    # group of rows where gender and person_event_group are matching\n",
    "    gender_correct = group['gender_letter'] == group['gender']\n",
    "    \n",
    "    if gender_correct.any() and len(group) > 1:\n",
    "        # if there is a dismatch only keep the matching\n",
    "        return group[gender_correct] if gender_correct.sum() > 0 else group.head(1)\n",
    "    else:\n",
    "        # if only one row or all are matching don't change anything\n",
    "        return group\n",
    "\n",
    "# use function on groups\n",
    "df_double_event = df_double_event.groupby(['event_link', 'person_link']).apply(resolve_gender_mismatch)\n",
    "\n",
    "# Optional: Index zurücksetzen\n",
    "df_double_event = df_double_event.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to before\n",
    "duplicates_grouped = df_double_event.groupby(['event_link', 'person_link'])['person_event_group'].apply(list).reset_index()\n",
    "\n",
    "print(f\"The total number of rows is {len(df_double_event)} and can be grouped to a \\n\"\n",
    "      f\"list with {len(duplicates_grouped)} entries of {duplicates_grouped['event_link'].nunique()} events\"\n",
    "      f\"\\nfrom {duplicates_grouped['person_link'].nunique()} individual athletes\"\n",
    "      # check if each athlete is there at least two times\n",
    "      f\"\\neach athlete: {df_double_event.groupby(['event_link', 'person_link']).size().min()} times per event\")\n",
    "\n",
    "duplicates_grouped.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### integrate info to df_stats\n",
    "\n",
    "Allthough there are still doubled groups I integrate the information I got so far to the main Dataframe df_stats because I will head for the age-groups now and at least some of the doublage will be solved autmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows that has to be dropped from df_stats\n",
    "\n",
    "df_double_drop = df_stats[df_stats.duplicated(subset=['person_link', 'event_link'], keep=False)]\n",
    "print(f\"Doubled groups event+person in df_stats: {len(df_double_drop)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"after dropping in df_double_event rows: {len(df_double_event)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows that has to be dropped from df_stats\n",
    "\n",
    "df_double_drop = df_stats[df_stats.duplicated(subset=['person_link', 'event_link'], keep=False)]\n",
    "\n",
    "# Schritt 2: Lösche die Zeilen aus df_stats, die in df_double_drop enthalten sind\n",
    "df_stats_filtered = df_stats.merge(df_double_drop, how='left', indicator=True)\n",
    "df_stats_filtered = df_stats_filtered[df_stats_filtered['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "\n",
    "# Schritt 3: Füge df_double_event zu df_stats_filtered hinzu\n",
    "df_stats_updated = pd.concat([df_stats_filtered, df_double_event], ignore_index=True)\n",
    "\n",
    "# Kontrolle: Überprüfe die Anzahl der Zeilen vor und nach der Änderung\n",
    "print(f\"Original df_stats rows: {len(df_stats)}\")\n",
    "print(f\"Filtered df_stats rows: {len(df_stats_filtered)}\")\n",
    "print(f\"Updated df_stats rows: {len(df_stats_updated)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = df_stats_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep: age classes\n",
    "\n",
    "For building new age groups for visualisation, I first have to decide what grouping would be reasonable.\n",
    "\n",
    "First aproach would be\n",
    "- <20\n",
    "- 20-34\n",
    "- 35-44\n",
    "- 45-59\n",
    "- 60-74\n",
    "- ->75\n",
    "\n",
    "1. I'll build a new column age_group to extract the numeric information out of person_event_group.\n",
    "2. I'll build another new column new_age_class that then will group by our own system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. age_group: new column\n",
    "\n",
    "get only the age as a new column to compare and in the end drop the person_event_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to extract age_group\n",
    "def extract_age_group(person_event_group):\n",
    "    if pd.isna(person_event_group):\n",
    "        return None\n",
    "    \n",
    "    # 1. pattern \"number number - number number\" (f.e. \"20-24\")\n",
    "    match = re.search(r'(\\d{2}-\\d{2})', person_event_group)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # 2. pattern \"number number +\" (f.e. \"20+\")\n",
    "    match = re.search(r'(\\d{2}\\+)$', person_event_group)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "     # 3. pattern \"number number\" (f.e. \"17\", \"23\")\n",
    "    match = re.search(r'(\\d{2})$', person_event_group)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "     # 4. pattern for  \"U number number\" (f.e. \"U23\") or \"MU number number\" (z.B. \"MU17\")\n",
    "    match = re.search(r'(U\\d{2}|MU\\d{2})$', person_event_group)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # 5. pattern for \"letter number number\" (f.e. M17, F25) exept letter \"U\"\n",
    "    match = re.search(r'([MF])(\\d{2})$', person_event_group)\n",
    "    if match:\n",
    "        if match.group(1) == 'U':\n",
    "            return match.group(0)  # f.e. \"U23\"\n",
    "        else:\n",
    "            return match.group(2)  # just number f.e. \"17\" or \"25\"\n",
    "    \n",
    "    # 6. pattern \"> number number\" f.e. '<19' or '>60'\n",
    "    match = re.search(r'(<\\d+|\\d+>)$', person_event_group)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    return None\n",
    "\n",
    "# new column age_group\n",
    "df_stats['age_group'] = df_stats['person_event_group'].apply(extract_age_group)\n",
    "\n",
    "# print result\n",
    "df_stats[['person_event_group', 'special_category', 'age_group']].head(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many empty age_group\n",
    "\n",
    "print(f\"There are {df_stats['age_group'].isna().sum():,} rows without age-group\"\n",
    "      f\"\\nand {df_stats[df_stats['age_group'].isna()]['person_link'].nunique():,} individual athletes \" \n",
    "      f\"out of {df_stats['person_link'].nunique():,} in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these 52,000 have entries in special_category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all athletes without 'age_group'\n",
    "athletes_without_age_group = df_stats[df_stats['age_group'].isna()]['person_link'].unique()\n",
    "\n",
    "# check for entries in special_category\n",
    "num_without_special_category = df_stats[\n",
    "   df_stats['person_link'].isin(athletes_without_age_group) & \n",
    "    (df_stats['special_category'].isna() | (df_stats['special_category'] == \"None\"))\n",
    "]['person_link'].nunique()\n",
    "\n",
    "# total of athletes without age_group\n",
    "total_athletes_without_age_group = len(athletes_without_age_group)\n",
    "\n",
    "# print result\n",
    "if num_without_special_category == total_athletes_without_age_group:\n",
    "    print(\"ALl athletes have an entry in special_category.\")\n",
    "else:\n",
    "    print(f\"{num_without_special_category:,} of the {total_athletes_without_age_group:,} Athletes don't have informations in special_category.\\n\")\n",
    "\n",
    "# Subset DataFrame of Athletes without 'age_group' and 'special_category'\n",
    "df_missing_both = df_stats[df_stats['age_group'].isna() & (df_stats['special_category'].isna() | (df_stats['special_category'] == \"None\"))]\n",
    "\n",
    "df_missing_both.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What person_event_group entries are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of unclear groups is {df_missing_both['person_event_group'].nunique()}\")\n",
    "list(df_missing_both['person_event_group'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are Nones - let's check them first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_rows_count = df_missing_both['person_event_group'].isna().sum()\n",
    "unique_athletes_count = df_missing_both[df_missing_both['person_event_group'].isna()]['person_link'].nunique()\n",
    "\n",
    "print(f\"The number of 'None' rows is {none_rows_count} with {unique_athletes_count} affected athletes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_both.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do they have other information: gender & country\n",
    "\n",
    "# filter the rows where person_event_group is None\n",
    "df_missing_both_none_group = df_missing_both[df_missing_both['person_event_group'].isna()]\n",
    "\n",
    "# for both\n",
    "with_both = df_missing_both_none_group.dropna(subset=['gender', 'person_flag'])\n",
    "\n",
    "# for only gender\n",
    "with_gender_only = df_missing_both_none_group[df_missing_both_none_group['gender'].notna() & df_missing_both_none_group['person_flag'].isna()]\n",
    "\n",
    "# for only person_flag as country\n",
    "with_country_only = df_missing_both_none_group[df_missing_both_none_group['person_flag'].notna() & df_missing_both_none_group['gender'].isna()]\n",
    "\n",
    "\n",
    "# print reults\n",
    "print(f\"Number of 'None' rows but information on both 'gender' and 'country': {with_both.shape[0]}\")\n",
    "print(f\"Number of 'None' rows but information on at least 'gender': {with_gender_only.shape[0]}\")\n",
    "print(f\"Number of 'None' rows but information on at least 'country': {with_country_only.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get an overview how many events and participants lay behind these groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by event_link and aggregate the list of category names and number of unique person links\n",
    "grouped = df_missing_both.groupby('event_link').agg(\n",
    "    unique_groups=('person_event_group', 'unique'),\n",
    "    unique_person_links=('person_link', 'nunique')\n",
    ")\n",
    "\n",
    "# sort ascending by number of unique athletes per event\n",
    "grouped_sorted = grouped.sort_values(by='unique_person_links', ascending=False)\n",
    "\n",
    "# iterate through the rows \n",
    "print(f\"The {df_missing_both['event_link'].nunique():,} events with a total of {len(df_missing_both)} participations, \"\n",
    "      f\"\\ntheir corresponding group names and unique person counts \" \n",
    "      f\"out of {df_missing_both['person_link'].nunique():,} athletes are: \\n\")\n",
    "for event, row in grouped_sorted.iterrows():\n",
    "    # Filter out None values from unique_groups and convert the rest to strings\n",
    "    valid_groups = [str(group) for group in row['unique_groups'] if group is not None]\n",
    "    print(f\"Event: {event}\\nGroups: {', '.join(valid_groups)}\\nUnique person links: {row['unique_person_links']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do they have other information: gender & country\n",
    "\n",
    "# for both\n",
    "with_both = df_missing_both.dropna(subset=['gender', 'person_flag'])\n",
    "\n",
    "# for only gender\n",
    "with_gender_only = df_missing_both[df_missing_both['gender'].notna() & df_missing_both['person_flag'].isna()]\n",
    "\n",
    "# for only person_flag as country\n",
    "with_country_only = df_missing_both[df_missing_both['person_flag'].notna() & df_missing_both['gender'].isna()]\n",
    "\n",
    "\n",
    "# print reults\n",
    "print(f\"Out of {len(df_missing_both):,} total rows\")\n",
    "print(f\"Number of rows but information on both 'gender' and 'country': {with_both.shape[0]:,}\")\n",
    "print(f\"Number of rows but information on at least 'gender': {with_gender_only.shape[0]}\")\n",
    "print(f\"Number of rows but information on at least 'country': {with_country_only.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So they all have information for gender and country and I'll keep them without age-information by building an entry 'no age'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. new_age_class : new system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of entries in age_group is {df_stats['age_group'].nunique()}\")\n",
    "list(df_stats['age_group'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the agegroups will pose some problems as they aren't unambiguously but I group them by myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building lists per new_age_class - after checking in the next cell, for counts, \n",
    "# I integrate some ambigous groups to a class - f.e. 30-39 to list_35-44\n",
    "\n",
    "\n",
    "list_U20 = [\n",
    "    'U20', 'U19', '19', '18-19', '18-20','17', '17-18', '17-19', \n",
    "    '17-20',  '16', '16-17', '16-18', '16-19',  '15', '15-16', '15-17', '15-18', '15-19',  '14-15', '14-16', \n",
    "    '14-17', '14-19', '14-20',  '12-14', '12-15', '12-18', '10-14', \n",
    "    '10-16', '10-18', '01-19'\n",
    "]\n",
    "\n",
    "list_20_34 = [\n",
    "    '18-22', '18-23', '18-24', '18-25', '18-29','18-30', '18-34',\n",
    "    '20', '20-22', '20-24', '20-25', '20-29', '20-34', '20-39', '21', '21-30', '21-39', \n",
    "    '22', '23', '23-29', '23-34', '23-39', '24', '24-29', '24-34', '24-39', '25', \n",
    "    '25-29', '25-30', '25-31', '25-32', '25-33', '25-34', '25-35', '25-36', '25-38', \n",
    "    '25-39',  '30', '30+','30-34', '30-35', '30-36', '30-37'\n",
    "]\n",
    "\n",
    "list_35_44 = [\n",
    "    '30-39', '35', '35+','35-39', '35-40', '35-41', '35-42', '35-43', '35-44', '35-45', '35-46', \n",
    "    '35-47' \n",
    "]\n",
    "\n",
    "list_45_59 = [\n",
    "    '40', '40+','40-44', '40-45', '40-46', '40-47', '40-48', '40-49', '40-50', '40-51', \n",
    "    '40-52', '40-53', '40-54', '40-55', '40-56', '40-57', '40-58', '40-59', '40-60', \n",
    "    '40-62', '40-63',  '41-50', '44', '45', '45+', \n",
    "    '45-49', '45-50', '45-51', '45-53', '45-54', '45-55', '45-57', '45-59', '50', '50+',\n",
    "    '50-53', '50-54', '50-55', '50-56', '50-57', '50-58', '50-59', '50-60', \n",
    "    '50-61', '50-62', '55', '55-59', '55-60', \n",
    "]\n",
    "\n",
    "list_60_74 = [\n",
    "     '60', \n",
    "    '60+','60-64', '60-65', '60-66', '60-67', '60-69', '60-70', '60-74', '65', '65+', '65-69', '65-70', '65-74'\n",
    "    '70', '70+', '70-74', \n",
    "    '70-75',\n",
    "]\n",
    "\n",
    "list_75_plus = [\n",
    "    '70-79', '70-99', '75', '75+', '75-79', '75-80', '75-99', '80', '80+', \n",
    "    '80-84', '80-99', '85-89', '90+', '90-94', '90-99'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problematic age groups that have no clear position in the new categories\n",
    "\n",
    "list_ambiguous = ['18-39', '18-44', '18-49', '18-50', '17-39', '17-24', '16-22', '16-24', '15-29', '14-24', '00+','16-29',\n",
    "                 '25-40', '25-41', '30-40', '30-41', '30-42', '30-43', '30-44', '30-49',\n",
    "                 '35-48', '35-49', '35-50', '35-52', '35-53', '35-54', '35-56', '35-59', '35-60', '35-61', '35-62', '35-63', \n",
    "                 '35-64', '35-65', '35-66',\n",
    "                 '40-64', '40-65', '40-68', '40-70', \n",
    "                 '50-64', '55+',  '55-64', '55-66', \n",
    "                 '65-59', '65-99'\n",
    "                 ]\n",
    "\n",
    "# filtered dataframe by ambiguous age_class\n",
    "df_filtered = df_stats[df_stats['age_group'].isin(list_ambiguous)]\n",
    "\n",
    "# total number of participations (=rows)\n",
    "total_rows = len(df_filtered)\n",
    "\n",
    "# total number of unique athletes\n",
    "unique_athletes = df_filtered['person_link'].nunique()\n",
    "\n",
    "# number of participation per age_group\n",
    "age_group_counts = df_filtered['age_group'].value_counts()\n",
    "\n",
    "# print results\n",
    "print(f\"The total number of participations is {total_rows} and the number of individual athletes is {unique_athletes}\\n\")\n",
    "print(age_group_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will integrate the list_ambiguous to 'no age', as it's about 5117 participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_no_age = [None, '18-39', '18-44', '18-49', '18-50', '17-39', '17-24', '16-22', '16-24', '15-29', '14-24', '00+','16-29',\n",
    "                 '25-40', '25-41', '30-40', '30-41', '30-42', '30-43', '30-44', '30-49',\n",
    "                 '35-48', '35-49', '35-50', '35-52', '35-53', '35-54', '35-56', '35-59', '35-60', '35-61', '35-62', '35-63', \n",
    "                 '35-64', '35-65', '35-66',\n",
    "                 '40-64', '40-65', '40-68', '40-70', \n",
    "                 '50-64', '55+',  '55-64', '55-66', \n",
    "                 '65-59', '65-99'\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_age(age):\n",
    "    if age in list_U20:\n",
    "        return 'U20'\n",
    "    elif age in list_20_34:\n",
    "        return '20-34'\n",
    "    elif age in list_35_44:\n",
    "        return '35-44'\n",
    "    elif age in list_45_59:\n",
    "        return '45-59'\n",
    "    elif age in list_60_74:\n",
    "        return '60-74'\n",
    "    elif age in list_75_plus:\n",
    "        return '75+'\n",
    "    else:\n",
    "        return 'no age'\n",
    "\n",
    "df_stats['new_age_class'] = df_stats['age_group'].apply(categorize_age)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep: gender for radial line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by event_link to get the total participants per event = basis for gender % for vis in tableau\n",
    "df_count = df_stats.groupby(['event_link', 'gender', 'year']).size().reset_index(name='total_count')\n",
    "\n",
    "# bring to df_stats\n",
    "df_stats = pd.merge(df_stats, df_count, on=['event_link', 'gender', 'year'], how='left')\n",
    "\n",
    "# check result\n",
    "df_stats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_stats.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mart: before uploading to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a leaner dataframe in tableau, I drop the unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the helping columns of prep staging\n",
    "drop_columns = ['age_group', 'is_special', 'is_multiple_categories', 'gender_letter']\n",
    "df_stats_clean = df_stats.drop(columns=[col for col in drop_columns if col in df_stats.columns])\n",
    "\n",
    "# change to a more logical order\n",
    "desired_order = ['event_link', 'total_count', 'year', 'distance', 'person_link', \n",
    "                 'gender', 'special_category', 'new_age_class', \n",
    "                 'person_country', 'person_event_group']\n",
    "\n",
    "df_stats_clean = df_stats_clean[[col for col in desired_order if col in df_stats.columns]]\n",
    "\n",
    "df_stats_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, types\n",
    "from sqlalchemy import text # to be able to pass string\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from dotenv import dotenv_values # to load the data from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data from .env file\n",
    "config = dotenv_values()\n",
    "\n",
    "# define variables for the login\n",
    "pg_user = config['AZURE_USER'] \n",
    "pg_host = config['AZURE_HOST']\n",
    "pg_port = config['AZURE_PORT']\n",
    "pg_db = config['AZURE_DB']\n",
    "pg_schema = config['AZURE_SCHEMA']\n",
    "pg_pass = config['AZURE_PASS']\n",
    "\n",
    "# build the URL\n",
    "url = f'postgresql://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}'\n",
    "\n",
    "# create the engine\n",
    "engine = create_engine(url, echo=False)\n",
    "engine.url\n",
    "\n",
    "# build the search path\n",
    "my_schema = pg_schema \n",
    "with engine.begin() as conn: \n",
    "    result = conn.execute(text(f'SET search_path TO {my_schema};'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_db_connection(engine):\n",
    "    try:\n",
    "        connection = engine.connect() # including 'connection' as variable to close the connection\n",
    "        print(\"Connection successful!\")\n",
    "        connection.close() # closing the connection\n",
    "        return True\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test the connection\n",
    "test_db_connection(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load DataFrames to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_clean.to_sql('df_mart_Triathlon', con=engine, schema='public', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# Session for transaction controll with sessionmaker\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# rollback if there is still a connection \n",
    "session.rollback()\n",
    "session.close()\n",
    "\n",
    "# end the session properly\n",
    "engine.dispose()\n",
    "engine = create_engine(url, echo=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure there are no problems due to an open connection, reset engine before uploading df_stats\n",
    "# isolate to autocommit = make sure that the connection is closing\n",
    "\n",
    "#engine.dispose()\n",
    "#engine = create_engine(url, isolation_level=\"AUTOCOMMIT\", echo=False)\n",
    "\n",
    "\n",
    "# as this dataset is bigger, chunk it into 1,000 blocks and set method to 'multi'\n",
    "\n",
    "# df_stats.to_sql('df_tri_stats', con=engine, schema='public', if_exists='replace', index=False, chunksize=1000, method='multi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
